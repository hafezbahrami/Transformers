# Background
* We want to read a text from Shakespeare as training dataset:
  * The training dataset is all Shakespeare concatenated together
  * Then, after training, we want to see if our Transformers can generate text similar to Shakespeare.
  * It is based on training transformers
  * We are basically trying to reproduce the GPT2, 2017, with 124M parameters
  * Good Video: https://www.youtube.com/watch?v=kCc8FmEb1nY&t=451s&ab_channel=AndrejKarpathy